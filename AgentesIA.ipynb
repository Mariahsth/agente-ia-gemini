{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJUDIywYm4CQ44CwtvYrCK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mariahsth/agente-ia-gemini/blob/main/AgentesIA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "langchain e langchain do googleAI-> framework par criar agentes de IA  \n",
        "\n",
        "google-generativeai -> SDK oficial do google, que vai fazer a interação com os modelos"
      ],
      "metadata": {
        "id": "-iIPRwi4yIQ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mbcaL2-SxmpD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b688f07-4b56-47d5-d8cb-c6467ca82e8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade langchain langchain-google-genai google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conectar ao Gemini\n",
        "(buscar API key no google AI Studio e colocar na variável de ambiente)"
      ],
      "metadata": {
        "id": "XOCi6RMD00KR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "Tdc1Sc4I0y3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparar modelo, temperatura e API KEY"
      ],
      "metadata": {
        "id": "JEgFCrH23nx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0,\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "r9c5rGay30Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# teste\n",
        "resp_test=llm.invoke(\"Quem é você?\")\n",
        "print(resp_test.content)"
      ],
      "metadata": {
        "id": "1q1-bQiW5mG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definir o contexto do agente\n",
        "Criar PROMPT para fazer triagem entre:  \n",
        "1- Responder usando RAG  \n",
        "2- Abrir ticket  \n",
        "3- Pedir mais informações  "
      ],
      "metadata": {
        "id": "YnZhM0ef6fOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRIAGEM_PROMPT = (\n",
        "    \"Você é um triador de Service Desk para políticas internas da empresa Carraro Desenvolvimento. \"\n",
        "    \"Dada a mensagem do usuário, retorne SOMENTE um JSON com:\\n\"\n",
        "    \"{\\n\"\n",
        "    '  \"decisao\": \"AUTO_RESOLVER\" | \"PEDIR_INFO\" | \"ABRIR_CHAMADO\",\\n'\n",
        "    '  \"urgencia\": \"BAIXA\" | \"MEDIA\" | \"ALTA\",\\n'\n",
        "    '  \"campos_faltantes\": [\"...\"]\\n'\n",
        "    \"}\\n\"\n",
        "    \"Regras:\\n\"\n",
        "    '- **AUTO_RESOLVER**: Perguntas claras sobre regras ou procedimentos descritos nas políticas (Ex: \"Posso reembolsar a internet do meu home office?\", \"Como funciona a política de alimentação em viagens?\").\\n'\n",
        "    '- **PEDIR_INFO**: Mensagens vagas ou que faltam informações para identificar o tema ou contexto (Ex: \"Preciso de ajuda com uma política\", \"Tenho uma dúvida geral\").\\n'\n",
        "    '- **ABRIR_CHAMADO**: Pedidos de exceção, liberação, aprovação ou acesso especial, ou quando o usuário explicitamente pede para abrir um chamado (Ex: \"Quero exceção para trabalhar 5 dias remoto.\", \"Solicito liberação para anexos externos.\", \"Por favor, abra um chamado para o RH.\").'\n",
        "    \"Analise a mensagem e decida a ação mais apropriada.\"\n",
        ")"
      ],
      "metadata": {
        "id": "Humo47oa6pth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testar prompt  \n",
        "-> Instalar biblioteca que auxilia a dar uma saída mais estruturada, como pydantic  \n",
        "-> Instalar biblioteca de dicionário de dados, como o typing"
      ],
      "metadata": {
        "id": "kNaEdXRU-ijh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal, List, Dict\n",
        "\n",
        "class TriagemOut(BaseModel):\n",
        "  decisao:Literal[\"AUTO_RESOLVER\", \"PEDIR_INFO\", \"ABRIR_CHAMADO\"]\n",
        "  urgencia: Literal[\"BAIXA\", \"MEDIA\", \"ALTA\"]\n",
        "  campos_faltantes: List[str]= Field(default_factory=list)"
      ],
      "metadata": {
        "id": "2MH9G2xf-a32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_triagem = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0,\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "bxrCXaexAuhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estruturar mensagens enviadas pelo modelo, diferenciando quando é mensagem do sistema e quando é mensagem humama"
      ],
      "metadata": {
        "id": "2aTUjQ64BVcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "triagem_chain = llm_triagem.with_structured_output(TriagemOut)\n",
        "\n",
        "def triagem (mensagem: str) -> Dict:\n",
        "  saida: TriagemOut = triagem_chain.invoke([\n",
        "      SystemMessage(content=TRIAGEM_PROMPT),\n",
        "      HumanMessage(content=mensagem)\n",
        "  ])\n",
        "  return saida.model_dump()\n"
      ],
      "metadata": {
        "id": "u0X12k9JBNkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testes = [\"Posso reembolsar a internet?\",\n",
        "          \"Posso ter mais do que 5 dias remoto?\",\n",
        "          \"Posso reembolsar cursos ou treinamentos da alura?\",\n",
        "          \"Quantas capivaras tem no Rio Pinheiros?\"\n",
        "          ]\n",
        "\n",
        "for msg_testes in testes:\n",
        "  print(f\"Pergunta: {msg_testes}\\n -> Resposta: {triagem(msg_testes)}\\n\" )"
      ],
      "metadata": {
        "id": "5d5VjDNYEd7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configurando RAG"
      ],
      "metadata": {
        "id": "EbUrVajKjwEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Instalações:\n",
        "langchain_community ->  vetorização, busca semântica, etc..  \n",
        "faiss-cpu ->  ver similaridade entre os pedaços de texto  \n",
        "langchain-text-splitter ->  quebrar texto em vários pedaços  \n",
        "pymupdf -> ler pdf"
      ],
      "metadata": {
        "id": "6sLi5eJWkN2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade langchain_community faiss-cpu langchain-text-splitters pymupdf\n"
      ],
      "metadata": {
        "id": "xbcyLaTnjy9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carregando arquivos da empresa"
      ],
      "metadata": {
        "id": "ZIGu7VDLv_2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "docs=[]\n",
        "\n",
        "for n in Path(\"/content/\").glob(\"*.pdf\"):\n",
        "  try:\n",
        "    loader = PyMuPDFLoader(str(n))\n",
        "    docs.extend(loader.load())\n",
        "    print(f\"Carregado arquivo {n.name} com sucesso\")\n",
        "  except Exception as e:\n",
        "    print(f\"Erro ao carregar arquivo {n.name}:{e}\")\n",
        "\n",
        "print(f\"Total de documentos carregados: {len(docs)}\")"
      ],
      "metadata": {
        "id": "0bJeJaJznRmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dividindo os pdfs em partes menores (chunks)"
      ],
      "metadata": {
        "id": "i_bQtbnPwEFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size = 300, chunk_overlap =30)\n",
        "\n",
        "chunks = splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "lnHbV6tZtjS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks"
      ],
      "metadata": {
        "collapsed": true,
        "id": "luCf7LosujLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in chunks:\n",
        "  print(chunk)\n",
        "  print(\"\\n-------------------------\\n\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nLzmGN2tvT9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings\n",
        "\n",
        "Transformando os chunks em vetores (para saber como cada um se relaciona)"
      ],
      "metadata": {
        "id": "Ord1jswZwR8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/gemini-embedding-001\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "5N29wtjWwncB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "vectorstore=FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity_score_threshold\",\n",
        "                                     search_kwargs={\"score_threshold\": 0.3, \"k\":4})"
      ],
      "metadata": {
        "id": "bXLIsnOX1Gyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "prompt_rag = ChatPromptTemplate.from_messages([\n",
        "        (\"system\",\n",
        "     \"Você é um Assistente de Políticas Internas (RH/IT) da empresa Carraro Desenvolvimento. \"\n",
        "     \"Responda SOMENTE com base no contexto fornecido. \"\n",
        "     \"Se não houver base suficiente, responda apenas 'Não sei'.\"),\n",
        "\n",
        "    (\"human\", \"Pergunta: {input}\\n\\nContexto:\\n{context}\")\n",
        "])\n",
        "\n",
        "document_chain= create_stuff_documents_chain(llm_triagem, prompt_rag)"
      ],
      "metadata": {
        "id": "5g7uSSTYFZaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Formatadores\n",
        "import re, pathlib\n",
        "\n",
        "def _clean_text(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
        "\n",
        "def extrair_trecho(texto: str, query: str, janela: int = 240) -> str:\n",
        "    txt = _clean_text(texto)\n",
        "    termos = [t.lower() for t in re.findall(r\"\\w+\", query or \"\") if len(t) >= 4]\n",
        "    pos = -1\n",
        "    for t in termos:\n",
        "        pos = txt.lower().find(t)\n",
        "        if pos != -1: break\n",
        "    if pos == -1: pos = 0\n",
        "    ini, fim = max(0, pos - janela//2), min(len(txt), pos + janela//2)\n",
        "    return txt[ini:fim]\n",
        "\n",
        "def formatar_citacoes(docs_rel: List, query: str) -> List[Dict]:\n",
        "    cites, seen = [], set()\n",
        "    for d in docs_rel:\n",
        "        src = pathlib.Path(d.metadata.get(\"source\",\"\")).name\n",
        "        page = int(d.metadata.get(\"page\", 0)) + 1\n",
        "        key = (src, page)\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        cites.append({\"documento\": src, \"pagina\": page, \"trecho\": extrair_trecho(d.page_content, query)})\n",
        "    return cites[:3]"
      ],
      "metadata": {
        "id": "WwGz329rMI-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perguntar_politica_RAG(pergunta:str) -> Dict:\n",
        "  docs_relacionados=retriever.invoke(pergunta)\n",
        "\n",
        "  if not docs_relacionados:\n",
        "    return {\"answer\" : \"Não sei.\",\n",
        "            \"citacoes\": [],\n",
        "            \"contexto_encontrado\":False}\n",
        "  answer = document_chain.invoke({\"input\": pergunta,\n",
        "                                  \"context\":docs_relacionados})\n",
        "\n",
        "  txt = (answer or \"\").strip()\n",
        "\n",
        "  if txt.rstrip(\".!?\") == \"Não sei\":\n",
        "        return {\"answer\" : \"Não sei.\",\n",
        "            \"citacoes\": [],\n",
        "            \"contexto_encontrado\":False}\n",
        "\n",
        "  return {\"answer\" : txt,\n",
        "            \"citacoes\": formatar_citacoes(docs_relacionados, pergunta),\n",
        "            \"contexto_encontrado\":True}\n"
      ],
      "metadata": {
        "id": "D6cUfAgrHP7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testes = [\"Posso reembolsar a internet?\",\n",
        "          \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
        "          \"Posso reembolsar cursos ou treinamentos da alura?\",\n",
        "          \"Quantas capivaras tem no Rio Pinheiros?\"\n",
        "          ]\n"
      ],
      "metadata": {
        "id": "VXKFcr6oJ5kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for msg_testes in testes:\n",
        "  resposta = perguntar_politica_RAG(msg_testes)\n",
        "  print(f\"Pergunta: {msg_testes}\\n -> Resposta: {resposta['answer']}\" )\n",
        "  if resposta['contexto_encontrado']:\n",
        "    print(f\" -> Citações: \\n\" )\n",
        "    for c in resposta['citacoes']:\n",
        "      print(f\"- Documento: {c['documento']}, Página: {c['pagina']} \\n Trecho: {c['trecho']}\")\n",
        "  print(\"-----------------------------------\\n\")\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QHKmSCWGKHjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "instalar langgraph -> utilizando grafos para estruturar o agente"
      ],
      "metadata": {
        "id": "W3pUpF0D5wl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade langgraph"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PnbvBYQj5N73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Optional\n",
        "\n",
        "class AgentState(TypedDict, total=False):\n",
        "  pergunta: str\n",
        "  triagem: Dict\n",
        "  resposta: Optional[str]\n",
        "  citacoes:List[dict]\n",
        "  rag_sucesso: bool\n",
        "  acao_final: str"
      ],
      "metadata": {
        "id": "4xDXa5yZ6NIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def node_triagem(state: AgentState) -> AgentState:\n",
        "  print(\"Excecutando nó de triagem...\")\n",
        "  return {\"triagem\": triagem(state[\"pergunta\"])}\n",
        "\n"
      ],
      "metadata": {
        "id": "l_bWk4pv8z8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def node_auto_resolver(state: AgentState) -> AgentState:\n",
        "  print(\"Excecutando nó de auto resolver...\")\n",
        "  resposta_rag=perguntar_politica_RAG(state[\"pergunta\"])\n",
        "\n",
        "  update:AgentState = {\n",
        "      \"resposta\":resposta_rag[\"answer\"],\n",
        "      \"citacoes\":resposta_rag.get('citacoes', []),\n",
        "      \"rag_sucesso\":resposta_rag[\"contexto_encontrado\"],\n",
        "  }\n",
        "\n",
        "  if resposta_rag[\"contexto_encontrado\"]:\n",
        "    update[\"acao_final\"] = \"AUTO_RESOLVER\"\n",
        "\n",
        "  return update"
      ],
      "metadata": {
        "id": "5ishrd5Z-R3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def node_pedir_info(state: AgentState) -> AgentState:\n",
        "  print(\"Excecutando nó de pedir info...\")\n",
        "\n",
        "  faltantes = state[\"triagem\"].get(\"campos_faltantes\", [])\n",
        "\n",
        "  if faltantes:\n",
        "    detalhe = \",\".join(faltantes)\n",
        "  else:\n",
        "    detalhe = \"Tema e contexto específico\"\n",
        "\n",
        "  return {\"resposta\": f\"Para avançar preciso que detalhe: {detalhe}\",\n",
        "          \"citacoes\": [],\n",
        "          \"acao_final\": \"PEDIR_INFO\"}"
      ],
      "metadata": {
        "id": "B7nBD16xAMs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def node_abrir_chamado(state: AgentState) -> AgentState:\n",
        "  print(\"Excecutando nó de pedir chamado...\")\n",
        "\n",
        "  triagem=state[\"triagem\"]\n",
        "\n",
        "  return{\n",
        "      \"resposta\":f\"Abrindo chamado com urgência {triagem['urgencia']}. Descrição: {state['pergunta'][:140]}\",\n",
        "      \"citacoes\":[],\n",
        "      \"acao_final\":\"ABRIR_CHAMADO\"\n",
        "  }"
      ],
      "metadata": {
        "id": "iP9xIN0KB7jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KEYWORDS_ABRIR_TICKET = [\"aprovação\", \"exceção\", \"liberação\", \"abrir ticket\", \"abrir chamado\", \"acesso especial\"]\n",
        "\n",
        "def decidir_pos_triagem(state: AgentState) -> str:\n",
        "  print(\"Decidindo após a triagem...\")\n",
        "  decisao=state[\"triagem\"][\"decisao\"]\n",
        "\n",
        "  if decisao == \"AUTO_RESOLVER\": return \"auto\"\n",
        "  if decisao == \"PEDIR_INFO\": return \"info\"\n",
        "  if decisao == \"ABRIR_CHAMADO\": return \"chamado\""
      ],
      "metadata": {
        "id": "EfWkHBB1DVvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decidir_pos_auto_resolver(state: AgentState) -> str:\n",
        "  print(\"Decidindo após o auto resolver...\")\n",
        "\n",
        "  if state.get(\"rag_sucesso\"):\n",
        "    print(\"Rag com sucesso, finalizando o fluxo\")\n",
        "    return \"ok\"\n",
        "\n",
        "  state_da_pergunta = (state[\"pergunta\"] or \"\").lower()\n",
        "\n",
        "  if any(k in state_da_pergunta for k in KEYWORDS_ABRIR_TICKET):\n",
        "    print(\"Rag falhou, mas foram encontradas keywords de abertura de ticket, abrindo...\")\n",
        "    return \"chamado\"\n",
        "\n",
        "  print(\"Rag falhou, sem keywords, vou pedir mais informações...\")\n",
        "  return \"info\"\n"
      ],
      "metadata": {
        "id": "KbgWoMjxFVhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"triagem\", node_triagem)\n",
        "workflow.add_node(\"auto_resolver\", node_auto_resolver)\n",
        "workflow.add_node(\"pedir_info\", node_pedir_info)\n",
        "workflow.add_node(\"abrir_chamado\", node_abrir_chamado)\n",
        "\n",
        "\n",
        "workflow.add_edge(START, \"triagem\")\n",
        "workflow.add_conditional_edges(\"triagem\", decidir_pos_triagem,\n",
        " {\"auto\": \"auto_resolver\",\n",
        "  \"info\":\"pedir_info\",\n",
        "  \"chamado\":\"abrir_chamado\"\n",
        "  })\n",
        "workflow.add_conditional_edges(\"auto_resolver\", decidir_pos_auto_resolver,\n",
        " {\n",
        "  \"info\":\"pedir_info\",\n",
        "  \"chamado\":\"abrir_chamado\",\n",
        "  \"ok\":END\n",
        "  })\n",
        "workflow.add_edge(\"pedir_info\", END)\n",
        "workflow.add_edge(\"abrir_chamado\", END)\n",
        "\n",
        "grafo=workflow.compile()"
      ],
      "metadata": {
        "id": "FWVNi3JyHlLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Image\n",
        "\n",
        "graph_bytes = grafo.get_graph().draw_mermaid_png()\n",
        "display(Image(graph_bytes))"
      ],
      "metadata": {
        "id": "EZe67d2FKlm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testes = [\"Posso reembolsar a internet?\",\n",
        "          \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
        "          \"Posso reembolsar cursos ou treinamentos da alura?\",\n",
        "          \"Tem como reembolsar certificações do Google Cloud?\",\n",
        "          \"Posso obter o Google Gemini de graça?\",\n",
        "          \"Qual é a palavra chave de hoje?\",\n",
        "          \"Quantas capivaras tem no Rio Pinheiros?\"\n",
        "          ]"
      ],
      "metadata": {
        "id": "OeUnapUhLL4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for msg_teste in testes:\n",
        "  resposta_final= grafo.invoke({\"pergunta\":msg_teste})\n",
        "\n",
        "  triag = resposta_final.get(\"triagem\", {})\n",
        "  print(f\"PERGUNTA: {msg_teste}\")\n",
        "  print(f\"DECISÃO: {triag.get('decisao')} | URGÊNCIA: {triag.get('urgencia')} | AÇÃO FINAL: {resposta_final.get('acao_final')}\")\n",
        "  print(f\"RESPOSTA: {resposta_final.get('resposta')}\")\n",
        "  if resposta_final.get('citacoes'):\n",
        "    print(\"CITAÇÕES: \")\n",
        "    for citacao in resposta_final.get('citacoes'):\n",
        "       print(f\"- Documento: {citacao['documento']}, Página: {citacao['pagina']} \\n Trecho: {citacao['trecho']}\")\n",
        "  print(\"-----------------------------------\\n\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BcueY6_8Lq-u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}